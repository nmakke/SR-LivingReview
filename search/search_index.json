{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Living Review of Symbolic Regression","text":"<p>Symbolic regression (SR) is a rapidly growing subfield of machine learning (ML) aiming to learn the analytical form of models that underlie data by searching the space of mathematical expressions. A growing interest in SR is taking place because it is naturally interpretable, i.e., it learns a transparent relationship between the input and the output, allowing for reasoning, in contrast to blackbox models learned by neural networks (NNs) where the input-output relationship is opaque and untractable.</p> <p>This document provides a categorized list of state-of-the-art methods, datasets, and applications of SR as part of the review entitled Interpretable Scientific Discovery with Symbolic Regression: A Review, which provides a technical and unified review of several research works on SR. This document's goal to list all research works so this list will continue to evolve. </p> <p>The purpose of this note is to make a sufficient and complete database for materials on SR (research papers, open-source codes, datasets, useful online resources, educational materials, etc.) for those developing and applying these approaches to different research areas.</p> <p></p> Reviews and Benchmarks <ul> <li>Interpretable Scientific Discovery with Symbolic Regression: A Review</li> <li>Contemporary Symbolic Regression Methods and their Relative Performance </li> </ul> Summary of SR Methods Category Brief description Underlying methods learned model Regression-based This category presumes a fixed model structure. The linear approach defines the model as a linear combination of non-linear functions and reduces SR to a system of linear equations, whereas the non-linear method relies on a multi-layer perceptron (MLP). In both cases, parameters are learned. Linear SR (sparse regression)  Non-linear SR System of linear equations  Multi-Layer Perceptron (MLP) Expression tree-based This category treats mathematical equations as unary-binary trees whose internal nodes are mathematical operators (algebraic operators, analytical functions) and terminal nodes are constants and state variables. Genetic programming (GP)  Reinforcement learning (RL)  Transformer neural network (TNN) Tree structure  Policy  Seq2seq models Physics-inspired This category takes into account the units of measurements of physical variables (so-called dimensional analysis) to constraint the search space Deep learning  polynomial fit  brute force search Neural network parameters  polynomial coefficients Mathematics-inspired This method uses the Meijer functions General mathematical function parameters of the Meijer functions Summary of SR Datasets <p>Data sets (\\(\\mathcal{D}\\)) are categorized into two main groups:</p> <p>Synthetic data  for which the analytical form of the underlying model is known and used to generate data points.  Example: \\(f(x) = 2x^2 + \\cos(x) \\rightarrow \\mathcal{D}=(x_i,f(x_i))_{i=1}^{n}\\) for \\(x \\in [0,1]\\)</p> <p>Real-world data for which underlying models are unknown.</p> <p> Type Category Underlying model Benchmarks Number of equations Synthetic data Physics Physics equations  Revised AIFeynman 120 Physics equations AIFeynman 120 Ordinary differential equations Strogatz 10 Mathematics monomials, polynomials,  trigonometric, exponential,  logarithm, power law, etc. Koza3 Keijer15 Vladislavleva8 Nguyen12 Korns15 R3 Jin6 Livermore22 Real world data economy, climate, commerce, etc. NA Penn Machine Learning Benchmarks 419 </p>"},{"location":"#reviews","title":"Reviews","text":""},{"location":"#summary-of-sr-methods","title":"Summary of SR Methods","text":""},{"location":"#summary-of-sr-datasets","title":"Summary of SR Datasets","text":""},{"location":"applications/","title":"Applications","text":"<ul> <li> <p>Physics   </p> <ul> <li>Discovering Symbolic Models from Deep Learning with Inductive Biases <code>code</code> (GNN + SR)</li> <li>Data-driven discovery of coordinates and governing equations <code>code</code> (SINDY + AE)</li> <li>Rediscovering orbital mechanics with machine learning</li> <li>Back to the Formula -- LHC Edition</li> <li>SYMBA: SYMBOLIC COMPUTATION OF SQUARED AMPLITUDES IN HIGH ENERGY PHYSICS WITH MACHINE LEARNING <code>code</code></li> </ul> </li> <li> <p>Benchmark</p> </li> <li>Contemporary Symbolic Regression Methods and their Relative Performance <code>code</code></li> </ul>"},{"location":"cite/","title":"Cite","text":"<p>Please cite the review if you have found it useful</p> <pre><code>@misc{makke2023interpretable,\n      title={Interpretable Scientific Discovery with Symbolic Regression: A Review}, \n      author={Nour Makke and Sanjay Chawla},\n      year={2023},\n      eprint={2211.10873},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":"<p>Data sets, used for model training and testing, are categorized into two main groups:</p> Synthetic Data <p> </p> <p>For synthetic problems, the analytical form of underlying models is known and used to generate data points.  This category includes physics equations which are constrained by physical units.  Examples: mathematical equation: \\(f(x) = 2x^2 + \\cos(x)\\) physical equation: \\(f(x) = Gm_1m_2/r^2\\) with \\([m_1]=[m_2]=\\) Kilograms, \\([r]=\\) meter and \\(G\\) is gravitational constant.</p> Origin Underlying model Benchmark name Reference problems year Physics Ordinary differential equations  General physics equations (classical mechanics, electromagnetism,  quantum mechanics, gravity, nuclear physics, etc. ) Strogatz  AIFeynman Strogatz repositery Feynman Database 10  120 2011  2019  Mathematics  monomials, polynomials,  trigonometric, exponential, etc. Koza  Keijer  Vladislavleva  Nguyen  Korns  R  Jin  Livermore Koza  Keijer M. Vladislavleva E.J. et al. Uy N.Q. et al. Korns M.F.  Krawiec K., Pawlak T. Jin Y. et al. Petersen B. K. et al. 3  15  8  12  15  3  6  22 1994  2003  2009  2011  2011  2013  2019  2021 Real Data <p> </p> <p>For real-world problems, the underlying model is fully unknown. This category includes any type of data such as climate, economics, medical, etc.  Datasets for this category can be found in The Penn Machine Learning benchmarks (PMLB) directory.</p> Research on SR datasets <ul> <li>Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery 2022</li> <li>Expression Sampler as a Dynamic Benchmark for Symbolic Regression [DOI] 2023 </li> </ul> Examples of mathematical equations Dataset Expression Variables Training data range Koza-1 \\(x^4 + x^3 + x^2 + x\\) 1 U[-1, 1, 20] Koza-2 \\(x^5 - 2x^3 + x\\) 1 U[-1, 1, 20] Koza-3 \\(x^6 - 2x^4 + x^2\\) 1 U[-1, 1, 20] Keijzer-1 \\(0.3 x \\sin(2\\pi x)\\) 1 E[-1, 1, 0.1] Keijzer-2 \\(0.3 x \\sin(2\\pi x)\\) 1 E[-2, 2, 0.1] Keijzer-3 \\(0.3 x \\sin(2\\pi x)\\) 1 E[-3, 3, 0.1] Keijzer-4 \\(x^3e^{-x} \\cos(x)\\sin(x)(\\sin^2(x)\\cos(x)-1)\\) 1 E[0, 10, 0.05] Keijzer-5 \\(30xz/(x-10)y^2\\) 3 \\(x,z:\\) U[-1,1,1000]  \\(y:\\) U[1,2,1000] Keijzer-6 \\(\\sum_1^{x}i\\) 1 E[1, 50, 1] Keijzer-7 \\(\\log x\\) 1 E[1, 100, 1] Keijzer-8 \\(\\sqrt{x}\\) 1 E[0, 100, 1] Keijzer-9 \\(\\mathrm{arcsinh}(x)=\\log(x +\\sqrt{x^2 + 1})\\) 1 E[0, 100, 1] Keijzer-10 \\(x^y\\) 2 U[0, 1, 100] Keijzer-11 \\(xy + \\sin((x-1)(y-1))\\) 2 U[-3, 3, 20] Keijzer-12 \\(x^4-x^3 +y^2/2 - y\\) 2 U[-3, 3, 20] Keijzer-13 \\(6\\sin(x)\\cos(y)\\) 2 U[-3, 3, 20] Keijzer-14 \\(8/(2+x^2+y^2)\\) 2 U[-3, 3, 20] Keijzer-15 \\(x^3/5 +y^3/2-y-x\\) 2 U[-3, 3, 20] Vladislavleva-1 \\(\\frac{e^{-(x-1)^2}}{1.2+(y-2.5)^2}\\) 1 U[0.3, 4, 100] Vladislavleva-2 \\(e^{-x}x^3(\\cos x\\sin x)(\\cos x \\sin^2 x-1)\\) 2 E[0.5, 10, 0.1] Vladislavleva-3 \\(e^{-x}x^3(\\cos x\\sin x)(\\cos x\\sin^2 x-1)(y-5)\\) 2 \\(x:\\)E[0.05,10,0.1]  \\(y:\\)E[0.05,10.05,2] Vladislavleva-4 \\(\\frac{10}{5+\\sum_{i=1}^{5}(x_i-3)^2}\\) 5 U[0.05, 6.05, 1024] Vladislavleva-5 \\(30(x-1)\\frac{(z-1)}{y^2(x-10)}\\) 3 \\(x:\\) U[0.05, 2, 300]  \\(y:\\) U[1, 2, 300]  \\(z:\\) U[0.05, 2, 300] Vladislavleva-6 \\(6\\sin(x)\\cos(y)\\) 2 U[0.1, 5.9, 30] Vladislavleva-7 \\((x-3)(y-3) + 2\\sin((x-4)(y-4))\\) 2 U[0.05, 6.05, 300] Vladislavleva-8 \\(\\frac{(x-3)^4+(y-3)^3-(y-3)}{(y-2)^4+10}\\) 2 U[0.05, 6.05, 50] Nguyen-1 \\(x^3+ x^2 + x\\) 1 U(-1,1,20) Nguyen-2 \\(x^4 + x^3+ x^2 + x\\) 1 U(-1,1,20) Nguyen-3 \\(x^5 + x^4 + x^3+ x^2 + x\\) 1 U(-1,1,20) Nguyen-4 \\(x^6 + x^5 + x^4 + x^3+ x^2 + x\\) 1 U(-1,1,20) Nguyen-5 \\(\\sin(x^2)\\cos(x) -1\\) 1 U(-1,1,20) Nguyen-6 \\(\\sin(x) + \\sin(x+x^2)\\) 1 U(-1,1,20) Nguyen-7 \\(\\log(x+1) + \\log(x^2+1)\\) 1 U(0,2,20) Nguyen-8 \\(\\sqrt{x}\\) 1 U(0,4,20) Nguyen-9 \\(\\sin(x) + \\sin(y^2)\\) 2 U(-1,1,100) Nguyen-10 \\(2\\sin(x)\\cos(y)\\) 2 U(-1,1,100) Nguyen-11 \\(x^{y}\\) 2 Nguyen-12 \\(x^4 - x^3 + \\frac{1}{2}y^2 - y\\) 2 Korns-1 \\(1.57 + (24.3 v)\\) 1 U[-50, 50, 10000] Korns-2 \\(0.23 + 14.2\\frac{v+y}{3\\omega}\\) 3 U[-50, 50, 10000] Korns-3 \\(-5.41 + 4.9\\frac{v-x+y/w}{3\\omega}\\) 4 U[-50, 50, 10000] Korns-4 \\(-2.3 + 0.13\\sin(z)\\) 1 U[-50, 50, 10000] Korns-5 \\(3 + 2.13 \\ln(\\omega)\\) 1 U[-50, 50, 10000] Korns-6 \\(1.3 + 0.13 \\sqrt{x}\\) 1 U[-50, 50, 10000] Korns-7 \\(213.80940889(1- e^{-0.54723748542 x})\\) 1 U[-50, 50, 10000] Korns-8 \\(6.87 + 11 \\sqrt{7.23~x~v~\\omega}\\) 3 U[-50, 50, 10000] Korns-9 \\(\\frac{\\sqrt{x}}{\\ln(y)}\\frac{e^z}{v^2}\\) 4 U[-50, 50, 10000] Korns-10 \\(0.81 + 24.3\\frac{2 y+3 z^2}{4v^3+5\\omega^4}\\) 4 U[-50, 50, 10000] Korns-11 \\(6.87 + 11\\cos(7.23 x^3)\\) 1 U[-50, 50, 10000] Korns-12 \\(2-2.1\\cos(9.8 x)\\sin(1.3\\omega)\\) 2 U[-50, 50, 10000] Korns-13 \\(32-3\\frac{\\tan(x)}{\\tan(y)}\\frac{\\tan(z)}{\\tan(v)}\\) 4 U[-50, 50, 10000] Korns-14 \\(22-4.2(\\cos(x)-\\tan(y))\\frac{\\tanh(z)}{\\sin(v)}\\) 4 U[-50, 50, 10000] Korns-15 \\(12-6\\frac{\\tan(x)}{e^y}(\\ln(z)-\\tan(v))\\) 4 U[-50, 50, 10000] R1 \\((x+1)^3/(x^2-x+1)\\) 1 E[-1,1,20] R2 \\((x^5-3x^3+1)/(x^2+1)\\) 1 E[-1,1,20] R3 \\((x^6+x^5)/(x^4+x^3+x^2+x+1)\\) 1 E[-1,1,20] Jin-1 \\(2.5x^4 -1.3x^3 +0.5y^2 -1.7y\\) 2 U(-3,3,100) Jin-2 $ 8.0x^2 + 8.0y^3 -15.0$ 2 U(-3,3,100) Jin-3 $ 0.2x^3 +1.5y^3 -1.2y -0.5x$ 2 U(-3,3,100) Jin-4 $ 1.5\\exp(x) + 5.0\\cos(y)$ 2 U(-3,3,100) Jin-5 $ 6.0\\sin(x)\\cos(y)$ 2 U(-3,3,100) Jin-6 $ 1.35xy + 5.5\\sin((x-1.0)(y-1.0)$ 2 U(-3,3,100) Livermore-1 \\(1/3 + x + \\sin(x^2)\\) 1 U[-10,10,1000] Livermore-2 \\(\\sin(x^2)\\cos(x) - 2\\) 1 U[-1,1,20] Livermore-3 \\(\\sin(x^3)\\cos(x^2) -1\\) 1 U[-1,1,20] Livermore-4 \\(\\log(x+1) + \\log(x^2+1)+\\log(x)\\) 1 U[0,2,20] Livermore-5 \\(x^4 - x^3 + x^2 -y\\) 2 U[0,1,20] Livermore-6 \\(4x^4 + 3x^3 + 2x^2 + x\\) 1 U[-1,1,20] Livermore-7 \\(\\sinh(x)\\) 1 U[-1,1,20] Livermore-8 \\(\\cosh(x)\\) 1 U[-1,1,20] Livermore-9 \\(x^9 +x^8+x^7+x^6+x^5+x^4+x^3+x^2+x\\) 1 U[-1,1,20] Livermore-10 \\(6\\sin(x)\\cos(y)\\) 2 U[0,1,20] Livermore-11 \\(x^2y^2/(x+y)\\) 2 U[-1,1,50] Livermore-12 \\(x^5/y^3\\) 2 U[-1,1,50] Livermore-13 \\(x^{1/3}\\) 1 U[0,4,20] Livermore-14 \\(x^3+x^2+x+\\sin(x)+\\sin(x^2)\\) 1 U[-1,1,20] Livermore-15 \\(x^{1/5}\\) 1 U[0,4,20] Livermore-16 \\(x^{2/5}\\) 1 U[0,4,20] Livermore-17 \\(4\\sin(x)\\cos(y)\\) 2 U[0,1,20] Livermore-18 \\(\\sin(x^2)\\cos(x) - 5\\) 1 U[-1,1,20] Livermore-19 \\(x^5+x^4+x^2+x\\) 1 U[-1,1,20] Livermore-20 \\(\\exp(-x^2)\\) 1 U[-1,1,20] Livermore-21 \\(x^8+x^7+x^6+x^5+x^4+x^3+x^2+x\\) 1 U[-1,1,20] Livermore-22 \\(\\exp(-0.5x^2)\\) 1 U[-1,1,20]"},{"location":"datasets/#linear-approach","title":"Linear approach","text":""},{"location":"datasets/#linear-approach_1","title":"Linear approach","text":""},{"location":"methods/","title":"Methods","text":"<p>SR methods are grouped into different categories to be most useful. A minimal number of categories is chosen in order to be as simple and useful as possible. SR methods are listed for each category.</p>"},{"location":"methods/#regression-based-sr","title":"Regression-based SR","text":"Linear approaches <ul> <li>Discovering governing equations from data by sparse identification of nonlinear dynamical systems[DOI] (SINDY)</li> <li>Sparse identification of nonlinear dynamics for rapid model recovery [DOI]</li> <li>Data-driven discovery of coordinates and governing equations [DOI] <code>code</code> (SINDY_AE)</li> </ul> Non-linear approaches <ul> <li>AI Feynman: a Physics-Inspired Method for Symbolic Regression [DOI] <code>code</code> (AIFeynman)</li> <li>Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery</li> <li>Symbolic regression for scientific discovery: an application to wind speed forecasting</li> <li>Relational inductive biases, deep learning, and graph networks</li> <li>Extrapolation and learning equations (EQL)</li> <li>Learning Equations for Extrapolation and Control(EQL\\(_\\div\\))</li> </ul>"},{"location":"methods/#linear-approach","title":"Linear approach","text":""},{"location":"methods/#non-linear-approach","title":"Non-linear approach","text":""},{"location":"methods/#expression-tree-based-sr","title":"Expression tree-based SR","text":"Genetic programming (GP-based SR) <ul> <li>Eurequa</li> <li>PySR: High-Performance Symbolic Regression in Python and Julia</li> <li>Genetic programming as a means for programming computers by natural selection [DOI]</li> <li>Order of Nonlinearity as a Complexity Measure for Models Generated by Symbolic Regression via Pareto Genetic Programming [DOI]</li> <li>Improving Symbolic Regression with Interval Arithmetic and Linear Scaling [DOI]</li> <li>Accuracy in Symbolic Regression [DOI]</li> <li>Semantically-based crossover in genetic programming: application to real-valued symbolic regression [DOI]</li> <li>Rethinking Symbolic Regression: Morphology and adaptability for evolutionary algorithms [DOI]</li> </ul> Reinforcement learning (RL-based SR) <ul> <li>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients [DOI] <code>code</code>(DSR)</li> <li>Symbolic Regression via Neural-Guided Genetic Programming Population Seeding [DOI]<code>code</code> (DSR)</li> <li>Physical Symbolic Optimization <code>code</code> (\\(\\phi\\)-SO)</li> </ul> Transformer neural network (TNN-based SR) <ul> <li>End-to-end symbolic regression with transformers <code>code</code> (E2ET)</li> <li>Neural Symbolic Regression that Scales <code>code</code> (NeSymReS)</li> <li>A Generative Transformer Model for Symbolic Regression <code>code</code> (SymbolicGPT)</li> <li>SYMBA: Symbolic Computation of squared amplitudes in high energy physics with machine learning <code>code</code> (SYMBA)</li> <li>Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search (DGSR-MCTS)</li> <li>Controllable Neural Symbolic Regression (NSRwH)</li> <li>A Transformer Model for symbolic regression towards scientific discovery [DOI] <code>code</code></li> <li>ODEFormer: Symbolic regression of dynamical systems with transformer [DOI]<code>code</code> (ODEFormer)</li> <li>A Transformer Model for Symbolic Regression towards Scientific Discovery [DOI] <code>code</code><code>code</code>] (transformer4sr)</li> <li>Transformer-based Planning for Symbolic Regression [DOI] <code>code</code><code>code</code>] (TPSR)</li> </ul>"},{"location":"methods/#genetic-programming","title":"Genetic programming","text":""},{"location":"methods/#reinforcement-learning","title":"Reinforcement learning","text":""},{"location":"methods/#transformer-neural-network","title":"Transformer neural network","text":""},{"location":"methods/#other-sr-approaches","title":"Other SR approaches","text":"Physics-inspired <ul> <li>AI Feynman: a Physics-Inspired Method for Symbolic Regression [DOI] <code>code</code></li> <li>Physical Symbolic Optimization <code>code</code> (\\(\\phi\\)-SO)</li> </ul> Mathematics-inspired <ul> <li>Demystifying Black-box Models with Symbolic Metamodels [DOI] <code>code</code></li> </ul> Computational approach <ul> <li> <p>Distilling Free-Form Natural Laws from Experimental Data</p> </li> <li> <p>GFN-SR: Symbolic Regression with Generative Flow Networks [DOI] <code>code</code><code>code</code>]</p> </li> </ul>"},{"location":"methods/#physics-inspired","title":"Physics-inspired","text":""},{"location":"methods/#mathematics-inspired","title":"Mathematics-inspired","text":""},{"location":"methods/#computational-approach","title":"Computational approach","text":""},{"location":"ressources/","title":"Ressources","text":"<p>This directory includes open source codes of SR methods and useful online resources.</p> Open source codes <p>For method category, we use: EA=EVOLUTIONARY ALGORITHM (E.G., GP), DL=DEEP LEARNING, MIX=COMBINATION OF MULTIPLE CLASSES.</p> Method Category Code Implementation Year Brief description SINDy_AE Mix URL Python 2019 Formulates the SR problem as a system of linear equations built using the set of allowable operators and learns the coefficients of candidate functions using a deep autoencoder (AE) network.  This is an extendable version of the SINDy that learns a coordinate transformation of a reduced space where the dynamics are sparsely represented. EQL\\(\\div\\) DL (NN) URL Python 2018 A fully differentiable shallow neural network with non-linear activation functions (e.g., algebraic operators and analytical functions) instead of traditional ones (e.g., sigmoid, relu, softmax, etc.). This is an extendable version of the original EQL that includes the division among candidate activation functions. Eureqa EA URL 2009 It is a closed-source code that uses genetic programming to learn mathematical expressions. It was developed in 2009 and popularized as the scientific discovery machinery but dismisses its mission. PySR EA URL Pyhton/Julia 2023 Fast and parallelized symbolic regression in Python/Julia based on evolutionary algorithms (EA). Treats mathematical expressions as trees and uses tournament selection. PSTree EA URL Python 2022 Consists of an automated piece-wise non-linear regression method based on decision tree and genetic programming techniques. gplearn EA URL Pyhton - Koza-style symbolic regression in python. It includes a SymbolicRegressor, SymbolicClassifier, and SymbolicTransformer for different tasks. Operon EA URL C++ 2020 GP-based DSR Mix URL Pyhton 2021 Consists of a generative RNN model of symbolic expressions trained using reinforcement learning. NeSymReS DL (TNN) URL Pyhton 2021 A pre-trained transformer that predicts SR models directly from the data. Predicted models are then fine-tuned and the best is returned. E2ET DL (TNN) URL Pyhton 2022 A pre-trained transformer that predicts SR models directly from the data. Predicted models are then fine-tuned and the best is returned. \\(\\phi\\)-SO DL (RL) URL Python 2023 Consists of a generative RNN model of symbolic expressions trained using reinforcement learning (DSR method) while fully leveraging physical units (of measurement) constraints in the search space at two levels, external (in situ physical units constraints) and internal (during RNN training). AIFeynman Mix URL Pyhton 2019 A physics-informed SR method that uses NN to learn symmetries in data (translational, rotational, vibrational) to simply the global SR problem into simpler SR subproblems. SRBench - URL Python 2021 A benchmark consisting of 14 SR methods (among which DSR, BSR, AIFeynman, and FFX are non-EA-based methods). uDSR Mix URL Python 2022 A unified SR framework that includes AIFeynamn, DSR, linear model (LM), large-scale pre-training (LSPT), and GP altogether. This method takes the strength of each class of symbolic regression."},{"location":"ressources/#genetic-programming","title":"Genetic programming","text":""},{"location":"sr/","title":"Symbolic regression","text":"<p>SR is the task of learning an interpretable model using AI-based techniques. </p> <p>Note that AI-based models are often uninterpretable, meaning that the learned model is a formula that is either too complicated for any human to understand or proprietary, i.e., it is very hard to understand its inner workings and only its developers know the secrets of its formula.  SR folds in the category of interpretable AI because it aims to learn clear, transparent, and analytical expressions of underlying models. </p>"},{"location":"sr/#sr-problem-definition","title":"SR Problem definition","text":"<p>Given a data set \\(\\mathcal{D} =\\{\\mathbf{x}_i,y_i\\}_{i=1}^{n}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\), the objective is to learn the function \\(f(\\mathbf{x})\\) such that \\(y_i = f(\\mathbf{x}_i) ~~\\forall ~~i\\).</p> <p>Three components are required to solve the SR problem:</p> <ul> <li>a set of allowable mathematical operators, among which are algebraic operators \\(\\{+, -, \\times, \\div \\}\\),  analytical functions \\(\\{\\cos,\\sin,\\tan,\\exp,\\log,\\mathrm{pow},\\mathrm{sqrt},\\mathrm{etc}\\}\\)), constants \\(c\\), and state variables \\(x\\) for function composition commonly called  library \\(L\\)</li> <li>a loss function \\(l(f) = \\sum_i l(f(x_i),y_i))\\) </li> <li>an optimization problem to search for the optimal solution (\\(f^{*}\\)) over the space of mathematical expressions \\(\\mathcal{F}\\) as follows: \\(f^{*} = \\mathrm{argmin}_{f \\in\\mathcal{F}} l(f(\\mathbf{x}))\\)"},{"location":"sr/#challenges","title":"Challenges","text":"<ul> <li> <p>SR is an NP-hard problem  (Virgolin &amp; Pissis 2022). The number of mathematical expressions grows exponentially with the number of features. Suppose the target analytic expression has a length of 35 symbols and that the library consists of 15 different variables or operations (e.g., x, +, \u2212, \u00d7, /,sin, log, ...) to choose from for each symbol. A naive brute-force attempt to fit the dataset might then have to consider up to \\(15^{30}=1.9 10^{35}\\) trial solutions, which is obviously very challenging. The obvious conclusion one draws from these considerations is that symbolic regression requires one to develop highly efficient strategies to prune poor guesses.</p> </li> <li> <p>Discrete nature of its search space The optimization problem in SR is defined over the space of mathematical expressions, which is discrete. </p> </li> </ul>"},{"location":"srphysics/","title":"SR Meets Physics","text":"SR Applications in physics <ul> <li>Discovering Symbolic Models from Deep Learning with Inductive Biases <code>code</code> (GNN + SR)</li> <li>Data-driven discovery of coordinates and governing equations <code>code</code> (SINDY + AE)</li> <li>Rediscovering orbital mechanics with machine learning</li> <li>Back to the Formula -- LHC Edition</li> <li>[SYMBA: SYMB</li> </ul>"},{"location":"srphysics/#sr-applications","title":"SR Applications","text":""}]}